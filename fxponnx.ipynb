{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2-user4/.conda/envs/onnx/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-cteqaeqt because the default path (/home/gpu2-user4/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.onnx import register_custom_op_symbolic\n",
    "from torch.onnx.symbolic_helper import parse_args\n",
    "from models.seg_model_zoo import create_seg_model\n",
    "import sys \n",
    "# sys.path.append(\".\")\n",
    "# Path Definitions\n",
    "# pth_path = '/home/gpu2-user4/access/effvit/checkpoints/effvit/demo_256_256/demo_7411mIoU.pth'\n",
    "pth_path = '/home/gpu2-user4/access/effvit/checkpoints/effvit/demo_512_512/6225 mIoU_512_512.pth'\n",
    "onnx_path = '/home/gpu2-user4/access/effvit/onnx/effvit_b0_branch_simp.onnx'\n",
    "fxp_onnx_path = '/home/gpu2-user4/access/effvit/onnx/fxp_effvit_b0_512_512.onnx'\n",
    "npz_path = '/home/gpu2-user4/access/effvit/npz_logging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def clean_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "clean_directory(npz_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Scaling Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu2-user4/.conda/envs/onnx/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n",
      "idx_concat:  83\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 加载模型状态\n",
    "checkpoint = torch.load(pth_path, map_location=device)\n",
    "# input = torch.rand(1, 8, 256, 256).to(device)  # 把输入数据移到GPU上\n",
    "# input = torch.rand(1, 3, 256, 256).to(device)  # 把输入数据移到GPU上\n",
    "input = torch.rand(1, 3, 512, 512).to(device)  # 把输入数据移到GPU上\n",
    "\n",
    "# 创建模型并移到GPU上\n",
    "model = create_seg_model('quan_b0_demo', 'cityscapes').to(device)\n",
    "try:\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "model.eval()\n",
    "\n",
    "# 进行推理\n",
    "with torch.no_grad():\n",
    "    dummy_output = model(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start FXP ONNX Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(onnx_path)\n",
    "npz_idx = 0\n",
    "output_map = dict()\n",
    "input_map = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0: Conv\n",
      "Node 1: HardSwish\n",
      "Node 2: Slice\n",
      "Node 3: Slice\n",
      "Node 4: Slice\n",
      "Node 5: Slice\n",
      "Node 6: Conv\n",
      "Node 7: HardSwish\n",
      "Node 8: Conv\n",
      "Node 9: Add\n",
      "Node 10: Conv\n",
      "Node 11: HardSwish\n",
      "Node 12: Conv\n",
      "Node 13: Add\n",
      "Node 14: Conv\n",
      "Node 15: HardSwish\n",
      "Node 16: Conv\n",
      "Node 17: Add\n",
      "Node 18: Conv\n",
      "Node 19: HardSwish\n",
      "Node 20: Conv\n",
      "Node 21: Add\n",
      "Node 22: Conv\n",
      "Node 23: HardSwish\n",
      "Node 24: Conv\n",
      "Node 25: HardSwish\n",
      "Node 26: Conv\n",
      "Node 27: Conv\n",
      "Node 28: HardSwish\n",
      "Node 29: Conv\n",
      "Node 30: HardSwish\n",
      "Node 31: Conv\n",
      "Node 32: Add\n",
      "Node 33: Conv\n",
      "Node 34: HardSwish\n",
      "Node 35: Conv\n",
      "Node 36: HardSwish\n",
      "Node 37: Conv\n",
      "Node 38: Conv\n",
      "Node 39: HardSwish\n",
      "Node 40: Conv\n",
      "Node 41: HardSwish\n",
      "Node 42: Conv\n",
      "Node 43: Add\n",
      "Node 44: Conv\n",
      "Node 45: HardSwish\n",
      "Node 46: Conv\n",
      "Node 47: HardSwish\n",
      "Node 48: Conv\n",
      "Node 49: Conv\n",
      "Node 50: HardSwish\n",
      "Node 51: Conv\n",
      "Node 52: HardSwish\n",
      "Node 53: Conv\n",
      "Node 54: Add\n",
      "Node 55: Conv\n",
      "Node 56: HardSwish\n",
      "Node 57: Conv\n",
      "Node 58: HardSwish\n",
      "Node 59: Conv\n",
      "Node 60: Conv\n",
      "Node 61: HardSwish\n",
      "Node 62: Conv\n",
      "Node 63: HardSwish\n",
      "Node 64: Conv\n",
      "Node 65: Add\n",
      "Node 66: Conv\n",
      "Node 67: HardSwish\n",
      "Node 68: Conv\n",
      "Node 69: HardSwish\n",
      "Node 70: Conv\n",
      "Node 71: Conv\n",
      "Node 72: HardSwish\n",
      "Node 73: Conv\n",
      "Node 74: HardSwish\n",
      "Node 75: Conv\n",
      "Node 76: Conv\n",
      "Node 77: HardSwish\n",
      "Node 78: Conv\n",
      "Node 79: HardSwish\n",
      "Node 80: Conv\n",
      "Node 81: Conv\n",
      "Node 82: HardSwish\n",
      "Node 83: Conv\n",
      "Node 84: HardSwish\n",
      "Node 85: Conv\n",
      "Node 86: Concat\n",
      "Node 87: Conv\n",
      "Node 88: HardSwish\n",
      "Node 89: Conv\n",
      "Node 90: HardSwish\n",
      "Node 91: Conv\n",
      "Node 92: Add\n",
      "Node 93: Conv\n",
      "Node 94: HardSwish\n",
      "Node 95: Conv\n",
      "Node 96: HardSwish\n",
      "Node 97: Conv\n",
      "Node 98: Conv\n",
      "Node 99: Conv\n",
      "Node 100: Conv\n",
      "Node 101: Conv\n",
      "Node 102: Conv\n",
      "Node 103: Concat\n",
      "Node 104: Reshape\n",
      "Node 105: Transpose\n",
      "Node 106: Relu\n",
      "Node 107: Conv\n",
      "Node 108: Conv\n",
      "Node 109: Concat\n",
      "Node 110: Reshape\n",
      "Node 111: Transpose\n",
      "Node 112: Relu\n",
      "Node 113: Conv\n",
      "Node 114: Conv\n",
      "Node 115: Concat\n",
      "Node 116: Reshape\n",
      "Node 117: Transpose\n",
      "Node 118: Transpose\n",
      "Node 119: MatMul\n",
      "Node 120: ReduceSum\n",
      "Node 121: MatMul\n",
      "Node 122: MatMul\n",
      "Node 123: Div\n",
      "Node 124: Transpose\n",
      "Node 125: Reshape\n",
      "Node 126: Conv\n",
      "Node 127: Add\n",
      "Node 128: Conv\n",
      "Node 129: HardSwish\n",
      "Node 130: Conv\n",
      "Node 131: HardSwish\n",
      "Node 132: Conv\n",
      "Node 133: Add\n",
      "Node 134: Conv\n",
      "Node 135: Conv\n",
      "Node 136: Conv\n",
      "Node 137: Conv\n",
      "Node 138: Conv\n",
      "Node 139: Concat\n",
      "Node 140: Reshape\n",
      "Node 141: Transpose\n",
      "Node 142: Relu\n",
      "Node 143: Conv\n",
      "Node 144: Conv\n",
      "Node 145: Concat\n",
      "Node 146: Reshape\n",
      "Node 147: Transpose\n",
      "Node 148: Relu\n",
      "Node 149: Conv\n",
      "Node 150: Conv\n",
      "Node 151: Concat\n",
      "Node 152: Reshape\n",
      "Node 153: Transpose\n",
      "Node 154: Transpose\n",
      "Node 155: MatMul\n",
      "Node 156: ReduceSum\n",
      "Node 157: MatMul\n",
      "Node 158: MatMul\n",
      "Node 159: Div\n",
      "Node 160: Transpose\n",
      "Node 161: Reshape\n",
      "Node 162: Conv\n",
      "Node 163: Add\n",
      "Node 164: Conv\n",
      "Node 165: HardSwish\n",
      "Node 166: Conv\n",
      "Node 167: HardSwish\n",
      "Node 168: Conv\n",
      "Node 169: Add\n",
      "Node 170: Conv\n",
      "Node 171: HardSwish\n",
      "Node 172: Conv\n",
      "Node 173: HardSwish\n",
      "Node 174: Conv\n",
      "Node 175: Conv\n",
      "Node 176: Conv\n",
      "Node 177: Conv\n",
      "Node 178: Conv\n",
      "Node 179: Conv\n",
      "Node 180: Concat\n",
      "Node 181: Reshape\n",
      "Node 182: Transpose\n",
      "Node 183: Relu\n",
      "Node 184: Conv\n",
      "Node 185: Conv\n",
      "Node 186: Concat\n",
      "Node 187: Reshape\n",
      "Node 188: Transpose\n",
      "Node 189: Relu\n",
      "Node 190: Conv\n",
      "Node 191: Conv\n",
      "Node 192: Concat\n",
      "Node 193: Reshape\n",
      "Node 194: Transpose\n",
      "Node 195: Transpose\n",
      "Node 196: MatMul\n",
      "Node 197: ReduceSum\n",
      "Node 198: MatMul\n",
      "Node 199: MatMul\n",
      "Node 200: Div\n",
      "Node 201: Transpose\n",
      "Node 202: Reshape\n",
      "Node 203: Conv\n",
      "Node 204: Add\n",
      "Node 205: Conv\n",
      "Node 206: HardSwish\n",
      "Node 207: Conv\n",
      "Node 208: HardSwish\n",
      "Node 209: Conv\n",
      "Node 210: Add\n",
      "Node 211: Conv\n",
      "Node 212: Conv\n",
      "Node 213: Conv\n",
      "Node 214: Conv\n",
      "Node 215: Conv\n",
      "Node 216: Concat\n",
      "Node 217: Reshape\n",
      "Node 218: Transpose\n",
      "Node 219: Relu\n",
      "Node 220: Conv\n",
      "Node 221: Conv\n",
      "Node 222: Concat\n",
      "Node 223: Reshape\n",
      "Node 224: Transpose\n",
      "Node 225: Relu\n",
      "Node 226: Conv\n",
      "Node 227: Conv\n",
      "Node 228: Concat\n",
      "Node 229: Reshape\n",
      "Node 230: Transpose\n",
      "Node 231: Transpose\n",
      "Node 232: MatMul\n",
      "Node 233: ReduceSum\n",
      "Node 234: MatMul\n",
      "Node 235: MatMul\n",
      "Node 236: Div\n",
      "Node 237: Transpose\n",
      "Node 238: Reshape\n",
      "Node 239: Conv\n",
      "Node 240: Add\n",
      "Node 241: Conv\n",
      "Node 242: HardSwish\n",
      "Node 243: Conv\n",
      "Node 244: HardSwish\n",
      "Node 245: Conv\n",
      "Node 246: Add\n",
      "Node 247: Conv\n",
      "Node 248: Resize\n",
      "Node 249: Conv\n",
      "Node 250: Resize\n",
      "Node 251: Conv\n",
      "Node 252: Add\n",
      "Node 253: Add\n",
      "Node 254: Conv\n",
      "Node 255: HardSwish\n",
      "Node 256: Conv\n",
      "Node 257: HardSwish\n",
      "Node 258: Conv\n",
      "Node 259: Add\n",
      "Node 260: Conv\n",
      "Node 261: HardSwish\n",
      "Node 262: Conv\n"
     ]
    }
   ],
   "source": [
    "def has_attribute(attr_list, name):\n",
    "    has_attr = False\n",
    "    for a in attr_list:\n",
    "        if a.name == name:\n",
    "            has_attr = True\n",
    "            break\n",
    "    return has_attr\n",
    "def get_attribute(attr_list, name):\n",
    "    for a in attr_list:\n",
    "        if a.name == name:\n",
    "            return a\n",
    "    assert False\n",
    "def set_output_scale (node, output_scale):\n",
    "    for i in node.input:\n",
    "        if i in output_map: # 如果outputmap中有这个input，即和当前node相连的上一个node\n",
    "            for d in output_map[i]: # 遍历这个input的所有上一个node\n",
    "                if not has_attribute(d.attribute, \"output_scale\"): # 如果上一个node没有output_scale\n",
    "                    attr = onnx.helper.make_attribute(\"output_bitdepth\", 8) # 添加output_bitdepth\n",
    "                    d.attribute.append(attr)\n",
    "                    attr = onnx.helper.make_attribute(\"output_scale\", output_scale) # 添加output_scale\n",
    "                    d.attribute.append(attr) \n",
    "                    set_output_scale (d, output_scale) # 向上追溯没有output_scale的node,即输入输出scale相同的node\n",
    "# Initialization\n",
    "for node in model.graph.node:\n",
    "    node.doc_string = \"\"\n",
    "    if node.output[0] not in output_map:\n",
    "        output_map[node.output[0]] = []\n",
    "    output_map[node.output[0]].append(node)\n",
    "    for i in node.input:\n",
    "        if i not in input_map:\n",
    "            input_map[i] = []\n",
    "        input_map[i].append(node)\n",
    "        \n",
    "for i, node in enumerate(model.graph.node):\n",
    "    print(f\"Node {i}: {node.op_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npz_logging/107_mul.npz\n",
      "scale16.0 8.0 0.0078125\n",
      "npz_logging/108_mul.npz\n",
      "scale0.001953125 16.0 0.015625\n",
      "npz_logging/109_mul.npz\n",
      "scale0.0078125 16.0 0.00390625\n",
      "npz_logging/131_mul.npz\n",
      "scale8.0 8.0 0.0078125\n",
      "npz_logging/132_mul.npz\n",
      "scale0.001953125 16.0 0.015625\n",
      "npz_logging/133_mul.npz\n",
      "scale0.0078125 16.0 0.00390625\n",
      "npz_logging/160_mul.npz\n",
      "scale8.0 4.0 0.015625\n",
      "npz_logging/161_mul.npz\n",
      "scale0.0078125 16.0 0.03125\n",
      "npz_logging/162_mul.npz\n",
      "scale0.015625 16.0 0.00390625\n",
      "npz_logging/184_mul.npz\n",
      "scale8.0 4.0 0.015625\n",
      "npz_logging/185_mul.npz\n",
      "scale0.0078125 8.0 0.015625\n",
      "npz_logging/186_mul.npz\n",
      "scale0.015625 8.0 0.00390625\n",
      "Error occurred: [Errno 2] No such file or directory: '/home/gpu2-user4/access/effvit/npz_logging/201_add.npz'\n",
      "Error occurred: [Errno 2] No such file or directory: '/home/gpu2-user4/access/effvit/npz_logging/201_add.npz'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: \"output_scale\"\n",
       "type: FLOAT\n",
       "f: 4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for node in model.graph.node:\n",
    "    if node.op_type == \"Conv\":\n",
    "        npz_idx = npz_idx + 1\n",
    "        val = np.load(npz_path + \"/\" + str(npz_idx) + \"_conv.npz\")\n",
    "        input_scale = val[\"input_scale\"]\n",
    "        d = node\n",
    "        if d.input[0] in output_map: # 如果outputmap中有这个input，即和当前node相连的上一个node\n",
    "            if has_attribute(output_map[d.input[0]][0].attribute, \"output_scale\"): # 如果上一个node有output_scale\n",
    "                input_node_out_scale = get_attribute(output_map[d.input[0]][0].attribute, \"output_scale\").f # 获取上一个node的output_scale\n",
    "                input_scale[0] = 1/ input_node_out_scale # 计算当前node的input_scale\n",
    "                assert input_node_out_scale == 1/input_scale[0], str(input_node_out_scale) + \"-\" + str(1/input_scale[0]) # 检查是否相等\n",
    "            # 如果上一个node没有output_scale，那么就是输入的scale，那么当前node的input_scale就是1/输入的scale\n",
    "            set_output_scale (node, 1 / input_scale[0]) # 设置当前node的所有上一个node的output_scale\n",
    "        # 这里设置上一个node的input_scale \n",
    "        if node.input[0] in output_map:\n",
    "            for d in output_map[node.input[0]]:\n",
    "                if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                    attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                    d.attribute.append(attr)\n",
    "                    attr = onnx.helper.make_attribute(\"output_scale\", 1/input_scale[0])\n",
    "                    d.attribute.append(attr)  \n",
    "                \n",
    "        w = val[\"w\"]\n",
    "        weight_name = node.input[1]\n",
    "        ## edit weights\n",
    "        for idx, t in enumerate(model.graph.initializer):\n",
    "            if t.name == weight_name:\n",
    "                value = onnx.helper.make_tensor(t.name, onnx.TensorProto.FLOAT, t.dims, w.flatten())\n",
    "                model.graph.initializer.remove(t)\n",
    "                model.graph.initializer.insert(idx, value)\n",
    "                break\n",
    "        if \"b\" in val:\n",
    "            b = val[\"b\"]\n",
    "            bias_name = node.input[2]\n",
    "            value = onnx.helper.make_tensor(\"bias_\" + str(npz_idx), onnx.TensorProto.FLOAT, b.shape, b.flatten())\n",
    "            model.graph.initializer.append(value)\n",
    "            node.input[2] = \"bias_\" + str(npz_idx)\n",
    "            # ## edit weights\n",
    "            # for idx, t in enumerate(model.graph.initializer):\n",
    "            #     if t.name == bias_name:\n",
    "            #         print (bias_name, t.name, t.dims, b.flatten())\n",
    "            #         input()\n",
    "            #         value = onnx.helper.make_tensor(t.name, onnx.TensorProto.FLOAT, t.dims, b.flatten())\n",
    "            #         model.graph.initializer.remove(t)\n",
    "            #         model.graph.initializer.insert(idx, value)\n",
    "            #         break\n",
    "                \n",
    "        # del node.attribute[:]\n",
    "        attr = onnx.helper.make_attribute(\"input_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"input_scale\", 1/input_scale[0])\n",
    "        node.attribute.append(attr)   \n",
    "        attr = onnx.helper.make_attribute(\"weight_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"weight_ch_scales\", (1 / val[\"weight_scales\"]).flatten())\n",
    "        node.attribute.append(attr)   \n",
    "        attr = onnx.helper.make_attribute(\"bias_bitdepth\", 16)\n",
    "        node.attribute.append(attr)\n",
    "        \n",
    "    elif node.op_type == \"HardSwish\":\n",
    "        npz_idx = npz_idx + 1\n",
    "        val = np.load(npz_path + \"/\" + str(npz_idx) + \"_pwl.npz\")\n",
    "        \n",
    "        input_scale = 1/val[\"input_scale\"][0]\n",
    "        attr = onnx.helper.make_attribute(\"slopes\", val[\"slopes\"])\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"intercepts\", val[\"intercepts\"])\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"slopes_scale\", 1/val[\"slopes_scale\"][0])\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"intercepts_scale\", 1/val[\"intercepts_scale\"][0])\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"change_pts\", val[\"change_pts\"])\n",
    "        node.attribute.append(attr)\n",
    "        \n",
    "        if node.input[0] in output_map:\n",
    "            for d in output_map[node.input[0]]:\n",
    "                if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                    attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                    d.attribute.append(attr)\n",
    "                    attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input_scale\"][0])\n",
    "                    d.attribute.append(attr)  \n",
    "        # del node.attribute[:]\n",
    "        attr = onnx.helper.make_attribute(\"input_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"input_scale\", 1/val[\"input_scale\"][0])\n",
    "        node.attribute.append(attr)  \n",
    "         \n",
    "    elif node.op_type == \"Add\":\n",
    "        npz_idx = npz_idx + 1\n",
    "        try:\n",
    "            val = np.load(npz_path + \"/\" + str(npz_idx) +  \"_add.npz\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")  # 打印异常信息\n",
    "            npz_idx = npz_idx - 1   \n",
    "            continue\n",
    "            \n",
    "        for input1 in node.input:\n",
    "            if input1 in output_map:\n",
    "                for d in output_map[input1]:\n",
    "                    if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                        if d.op_type == \"Conv\":\n",
    "                            attr = onnx.helper.make_attribute(\"output_bitdepth\", 16)\n",
    "                        else:\n",
    "                            attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                        d.attribute.append(attr)\n",
    "                        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input1_scale\"][0])\n",
    "                        d.attribute.append(attr) \n",
    "    elif node.op_type == \"Concat\":\n",
    "        npz_idx = npz_idx + 1\n",
    "        val = np.load(npz_path + \"/\" + str(npz_idx) +  \"_concat.npz\")\n",
    "        for input1 in node.input:\n",
    "            if input1 in output_map:\n",
    "                for d in output_map[input1]:\n",
    "                    if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                        attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                        d.attribute.append(attr)\n",
    "                        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"scale_qkv\"][0])\n",
    "                        d.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"scale_qkv\"][0])\n",
    "        node.attribute.append(attr) \n",
    "    elif node.op_type == \"MatMul\":\n",
    "        # check if input 1 is a constant\n",
    "        for init in model.graph.initializer:\n",
    "            if init.name == node.input[1]:\n",
    "                # get the value as numpy array\n",
    "                w = onnx.numpy_helper.to_array(init)\n",
    "                print (w.shape)\n",
    "                # make a new tensor with the same name, data type, dimensions and values\n",
    "                \n",
    "                a = random.randint(0, 255)\n",
    "                \n",
    "                value = onnx.helper.make_tensor(init.name + \"_\" + node.name, onnx.TensorProto.FLOAT, init.dims, a * w.flatten())\n",
    "                # remove the old initializer and add the modified one\n",
    "                # model.graph.initializer.remove(init)\n",
    "                model.graph.initializer.append(value)\n",
    "                # change the input of the node to point to the new initializer\n",
    "                node.input[1] = init.name + \"_\" + node.name\n",
    "                break\n",
    "        npz_idx = npz_idx + 1\n",
    "        print (\"npz_logging/\" + str(npz_idx) + \"_mul.npz\")\n",
    "        val = np.load(npz_path + \"/\" + str(npz_idx) +  \"_mul.npz\")\n",
    "        d = output_map[node.input[0]][0]\n",
    "        attr = onnx.helper.make_attribute(\"A_bitdepth\", 8)\n",
    "        node.attribute.append(attr) \n",
    "        # attr = copy.deepcopy(get_attribute(d.attribute, \"output_scale\"))\n",
    "        # a_scale = attr.f\n",
    "        # attr.name = \"A_scale\"\n",
    "        # node.attribute.append(attr) \n",
    "        \n",
    "        attr = onnx.helper.make_attribute(\"A_scale\", 1/val[\"input_A_scale\"][0])\n",
    "        node.attribute.append(attr) \n",
    "        a_scale=1/val[\"input_A_scale\"][0]\n",
    "        if node.input[0] in output_map:\n",
    "            for d in output_map[node.input[0]]:\n",
    "                if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                    attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                    d.attribute.append(attr)\n",
    "                    attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input_A_scale\"][0])\n",
    "                    d.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"B_bitdepth\", 8)\n",
    "        node.attribute.append(attr) \n",
    "        # d = output_map[node.input[1]][0]\n",
    "        # attr = copy.deepcopy(get_attribute(d.attribute, \"output_scale\"))\n",
    "        # b_scale = attr.f\n",
    "        # attr.name = \"B_scale\"\n",
    "        # node.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"B_scale\", 1/val[\"input_B_scale\"][0])\n",
    "        node.attribute.append(attr) \n",
    "        b_scale=1/val[\"input_B_scale\"][0]\n",
    "        if node.input[1] in output_map:\n",
    "            for d in output_map[node.input[1]]:\n",
    "                if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                    attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                    d.attribute.append(attr)\n",
    "                    attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input_B_scale\"][0])\n",
    "                    d.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "        node.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"output_scale\"][0])\n",
    "        assert b_scale * a_scale >= 1/val[\"output_scale\"][0], \"scale\" + str(b_scale) + \" \" + str(a_scale) + \" \" + str(1/val[\"output_scale\"][0])\n",
    "        \n",
    "        print (\"scale\" + str(b_scale) + \" \" + str(a_scale) + \" \" + str(1/val[\"output_scale\"][0]))\n",
    "        \n",
    "        node.attribute.append(attr) \n",
    "    \n",
    "    elif node.op_type == \"Resize\":\n",
    "        npz_idx = npz_idx + 1\n",
    "        val = np.load(npz_path + \"/\" + str(npz_idx) + \"_resize.npz\")\n",
    "        for input1 in node.input:\n",
    "            if input1 in output_map:\n",
    "                for d in output_map[input1]:\n",
    "                    if not has_attribute(d.attribute, \"output_scale\"):\n",
    "                        attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "                        d.attribute.append(attr)\n",
    "                        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input_scale\"][0])\n",
    "                        d.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"input_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"input_scale\", 1/val[\"input_scale\"][0])\n",
    "        node.attribute.append(attr) \n",
    "        attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "        node.attribute.append(attr)\n",
    "        attr = onnx.helper.make_attribute(\"output_scale\", 1/val[\"input_scale\"][0])\n",
    "        node.attribute.append(attr) \n",
    "    \n",
    "    elif node.op_type == \"Div\":\n",
    "        npz_idx = npz_idx + 1\n",
    "    \n",
    "    elif node.op_type == \"ReduceSum\":\n",
    "        None\n",
    "    else:\n",
    "        d = output_map[node.input[0]][0]\n",
    "        if has_attribute(d.attribute, \"output_scale\"):\n",
    "            attr = copy.deepcopy(get_attribute(d.attribute, \"output_scale\"))\n",
    "            node.attribute.append(attr) \n",
    "            attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "            node.attribute.append(attr)    \n",
    "        \n",
    "attr = onnx.helper.make_attribute(\"output_bitdepth\", 8)\n",
    "node.attribute.append(attr)\n",
    "attr = onnx.helper.make_attribute(\"output_scale\", 4.0)\n",
    "node.attribute.append(attr)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the Final FXP ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save( model, fxp_onnx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
