{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "from re import A\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "from sympy import per\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.utils import get_same_padding, resize, val2list, val2tuple, merge_tensor\n",
    "from models.nn.norm import build_norm\n",
    "from models.nn.act import build_act, Quanhswish\n",
    "from models.nn.quant_lsq import QuanConv, PActFn, PACT, SymmetricQuantFunction\n",
    "from models.nn.lsq import LsqQuantizer4input, LsqQuantizer4weight\n",
    "from models.nn.ops import ConvLayer, DSConv, MBConv, EfficientViTBlock, OpSequential, ResidualBlock, IdentityLayer, LiteMSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, per_channel=True,\n",
    "                 dilation=1, groups=1, bias=True, input_bitdepth=8, weight_bitdepth=8, output_bitdepth=8):\n",
    "        super(Conv, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            groups, bias)\n",
    "        self.fixBN = True\n",
    "        self.per_channel = per_channel\n",
    "        # scale\n",
    "        self.input_scale = torch.ones(1, requires_grad=False) # 2^N\n",
    "        # reciprocal format (c / 2^N)\n",
    "        self.weight_scale = torch.ones(1, requires_grad=False) if self.per_channel \\\n",
    "                       else torch.ones(self.weight.shape[0], requires_grad=False)\n",
    "        self.output_scale = torch.ones(1, requires_grad=False)\n",
    "        # bit-width\n",
    "        self.input_bitdepth = input_bitdepth\n",
    "        self.weight_bitdepth = weight_bitdepth\n",
    "        self.output_bitdepth = output_bitdepth\n",
    "    def forward(self, input): # the value of input is int8 but the type is float32\n",
    "        # weight \n",
    "        w = self.weight / self.weight_scale # stored as Sw*W_int in fp format\n",
    "        # bias\n",
    "        b = self.bias / (self.weight_scale * self.input_scale) if self.bias is not None else None\n",
    "        # perform convolution in fp format to simulate integer convolution\n",
    "        output = F.conv2d(input, w, b,\n",
    "                          self.stride, self.padding, self.dilation, self.groups)\n",
    "        # dyadic scale\n",
    "        dyadic_scale = self.input_scale * self.weight_scale / self.output_scale\n",
    "        # final output\n",
    "        output = output * dyadic_scale \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        padding=None,\n",
    "        groups=1,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0,\n",
    "        norm=\"bn2d\",\n",
    "        act_func=\"relu\",\n",
    "        per_channel=True,\n",
    "        quan_a='acy',\n",
    "        quan_w='lsq',\n",
    "        nbit_w=8,\n",
    "        nbit_a=8,\n",
    "        res = False,\n",
    "        psum_quan=False,\n",
    "        cg=32\n",
    "    ):\n",
    "        super(QConvLayer, self).__init__()\n",
    "\n",
    "        padding = padding if padding else get_same_padding(kernel_size)\n",
    "        padding *= dilation\n",
    "        self.res = res\n",
    "        self.dropout = nn.Dropout2d(dropout_rate, inplace=False) if dropout_rate > 0 else None\n",
    "        self.quan_a_name = quan_a\n",
    "        if self.quan_a_name == 'acy':\n",
    "            self.pact = PACT(nbit_a)\n",
    "        self.conv = QuanConv(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            quan_name_w=quan_w,\n",
    "            quan_name_a=self.quan_a_name,\n",
    "            nbit_w=nbit_w,\n",
    "            nbit_a=nbit_a,\n",
    "            per_channel=per_channel,\n",
    "            stride=(stride, stride),\n",
    "            padding=padding,\n",
    "            dilation=(dilation, dilation),\n",
    "            groups=groups,\n",
    "            bias=use_bias,\n",
    "            norm=norm,\n",
    "            psum_quan=psum_quan,\n",
    "            cg=cg,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.quan_a_name == 'acy' and self.res == False:\n",
    "            x, scale_a = self.pact(x)\n",
    "            self.conv.scale_a = scale_a.detach()\n",
    "        elif self.quan_a_name == 'lsq' and self.res == False:\n",
    "            x, scale_a = self.conv.lsq_a(x)\n",
    "            self.conv.scale_a = scale_a.detach()\n",
    "\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = QConvLayer(3,\n",
    "                         16,\n",
    "                         kernel_size=3,\n",
    "                         stride=2,\n",
    "                         padding=1,\n",
    "                         norm='bn2d',\n",
    "                         act_func='relu',\n",
    "                         per_channel=True,\n",
    "                         quan_a='acy',\n",
    "                         quan_w='lsq',\n",
    "                         nbit_w=8,\n",
    "                         nbit_a=8,\n",
    "                         res=False,\n",
    "                         psum_quan=False,\n",
    "                         cg=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmaxformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
